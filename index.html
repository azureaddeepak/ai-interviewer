<script>
  const questions = [
    "Hello! I'm your AI interviewer. Can you explain what CI/CD is and how you've used it in Azure DevOps?",
    "Have you used Terraform or ARM templates? Please describe your experience.",
    "Tell me about a time you fixed a broken deployment pipeline."
  ];

  let currentQ = 0;
  const synth = window.speechSynthesis;
  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  const recognition = SpeechRecognition ? new SpeechRecognition() : null;

  const startBtn = document.getElementById("startBtn");
  const listenBtn = document.getElementById("listenBtn");
  const questionEl = document.getElementById("question");
  const statusEl = document.getElementById("status");
  const transcriptEl = document.getElementById("transcript");

  // Check for speech support
  if (!recognition) {
    statusEl.textContent = "Your browser doesn't support voice recognition. Try Chrome or Edge.";
    startBtn.disabled = true;
    return;
  }

  // Text-to-Speech
  function speak(text) {
    const utter = new SpeechSynthesisUtterance(text);
    utter.rate = 0.9;
    synth.speak(utter);
  }

  // Start interview with mic permission
  startBtn.onclick = async () => {
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      statusEl.textContent = "Microphone not supported.";
      return;
    }

    try {
      await navigator.mediaDevices.getUserMedia({ audio: true });
      statusEl.textContent = "Microphone granted. Starting interview...";
      startBtn.disabled = true;
      nextQuestion();
    } catch (err) {
      statusEl.textContent = "Microphone access denied. Please allow it.";
    }
  };

  // Move to next question
  function nextQuestion() {
    if (currentQ >= questions.length - 1) {
      questionEl.textContent = "Interview complete!";
      statusEl.textContent = "You can review your answers above.";
      return;
    }

    const q = questions[currentQ];
    questionEl.textContent = q;
    speak(q);

    statusEl.textContent = "Listening in 3 seconds...";
    setTimeout(startListening, 3000);
  }

  // Start listening
  function startListening() {
    recognition.onresult = (event) => {
      const transcript = event.results[0][0].transcript;
      transcriptEl.textContent += "\nA: " + transcript + "\n";
      statusEl.textContent = "Answer recorded.";
    };

    recognition.onerror = (event) => {
      statusEl.textContent = "Error: " + event.error;
    };

    recognition.start();

    // Stop after 5 seconds
    setTimeout(() => {
      recognition.stop();
      statusEl.textContent = "Listening ended. Moving to next question...";
      currentQ++;
      setTimeout(nextQuestion, 1000); // Wait 1 second before next
    }, 5000);
  }
</script>
