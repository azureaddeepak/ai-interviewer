<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>AI Interviewer (Azure DevOps)</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      padding: 50px;
      background-color: #f7f9fc;
    }
    button {
      font-size: 16px;
      padding: 12px 24px;
      margin: 10px;
      border: none;
      border-radius: 5px;
      background: #0078d4;
      color: white;
      cursor: pointer;
    }
    button:disabled {
      opacity: 0.5;
    }
    #status {
      margin: 20px;
      font-style: italic;
      color: #666;
    }
    pre {
      white-space: pre-wrap;
      background: #fff;
      padding: 15px;
      border-radius: 5px;
      max-width: 800px;
      margin: 20px auto;
      border: 1px solid #ddd;
    }
  </style>
</head>
<body>

  <h1>ü§ñ AI Interviewer (Azure DevOps)</h1>
  <p id="question">Press "Start" to begin your interview.</p>
  <button id="startBtn">üéôÔ∏è Start Interview</button>
  <div id="status"></div>
  <pre id="transcript"></pre>

  <script>
    // Interview Questions (Azure DevOps + AKS)
    const questions = [
      "Hello! I'm your AI interviewer. Can you explain what CI/CD is and how you've used it in Azure DevOps?",
      "Have you used Terraform or ARM templates? Please describe your experience.",
      "Tell me about a time you fixed a broken deployment pipeline.",
      "How do you manage secrets in Azure DevOps pipelines? Have you used Azure Key Vault?",
      "Explain how you deploy applications to Azure Kubernetes Service (AKS) using Azure DevOps.",
      "What tools do you use for monitoring and logging in AKS? How do you set up alerts?",
      "Describe your experience with Helm charts in AKS deployments."
    ];

    let currentQ = 0;
    const synth = window.speechSynthesis;
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    const recognition = SpeechRecognition ? new SpeechRecognition() : null;

    const startBtn = document.getElementById("startBtn");
    const questionEl = document.getElementById("question");
    const statusEl = document.getElementById("status");
    const transcriptEl = document.getElementById("transcript");

    // Check if SpeechRecognition is supported
    if (!recognition) {
      statusEl.textContent = "‚ö†Ô∏è Your browser doesn't support voice recognition. Please use Google Chrome.";
      startBtn.disabled = true;
    }

    // Add click event with delay to ensure DOM is ready
    setTimeout(() => {
      startBtn.onclick = function() {
        console.log("Start button clicked!"); // Debug log
        requestMicAndStart();
      };
    }, 500);

    // Request microphone and start interview
    function requestMicAndStart() {
      navigator.mediaDevices.getUserMedia({ audio: true })
        .then(stream => {
          statusEl.textContent = "üé§ Microphone access granted. Starting interview...";
          startBtn.disabled = true;
          nextQuestion();
        })
        .catch(err => {
          statusEl.textContent = "‚ùå Microphone access denied. Please allow it in browser settings.";
          console.error("Mic error:", err);
        });
    }

    // Speak and ask next question
    function nextQuestion() {
      if (currentQ >= questions.length) {
        questionEl.textContent = "‚úÖ Interview complete!";
        statusEl.textContent = "You can review your answers above.";
        return;
      }

      const q = questions[currentQ];
      questionEl.textContent = q;
      
      // Speak the question
      const utter = new SpeechSynthesisUtterance(q);
      utter.rate = 0.9;
      synth.speak(utter);

      statusEl.textContent = "üéôÔ∏è Listening in 3 seconds...";
      setTimeout(startListening, 3000);
    }

    // Start listening to answer
    function startListening() {
      try {
        recognition.onresult = function(event) {
          const transcript = event.results[0][0].transcript;
          transcriptEl.textContent += `\nQ: ${questions[currentQ]}\nA: ${transcript}\n`;
          statusEl.textContent = "‚úÖ Answer recorded.";
        };

        recognition.onerror = function(event) {
          statusEl.textContent = "Error: " + event.error;
        };

        recognition.start();

        // Stop after 5 seconds
        setTimeout(() => {
          recognition.stop();
          currentQ++;
          setTimeout(nextQuestion, 1000);
        }, 5000);

      } catch (err) {
        statusEl.textContent = "Failed to start listening: " + err.message;
      }
    }
  </script>
</body>
</html>
