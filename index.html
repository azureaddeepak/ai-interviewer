<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>AI Interviewer (Azure DevOps)</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      padding: 50px;
      background-color: #f7f9fc;
    }
    button {
      font-size: 16px;
      padding: 12px 24px;
      margin: 10px;
      border: none;
      border-radius: 5px;
      background: #0078d4;
      color: white;
      cursor: pointer;
    }
    button:disabled { opacity: 0.5; }
    .section { margin: 30px; }
    audio { margin: 10px; }
    #status { margin: 20px; font-style: italic; color: #666; }
  </style>
</head>
<body>

  <h1>üéôÔ∏è AI Interviewer (Azure DevOps + AKS)</h1>
  <p id="question">Press "Start" to begin.</p>

  <div class="section">
    <button id="startBtn">üü¢ Start Interview</button>
    <button id="recordBtn" disabled>üî¥ Record Answer (30 sec)</button>
    <button id="nextBtn" disabled>‚û°Ô∏è Next Question</button>
  </div>

  <div id="status">Ready when you are.</div>
  <div id="recordings"></div>

  <script>
    const questions = [
      "Welcome! Please tell me about your experience with Azure DevOps and CI/CD pipelines.",
      "Have you used Terraform or ARM templates? Describe your experience.",
      "Tell me about a time you fixed a broken deployment pipeline.",
      "How do you manage secrets in Azure DevOps? Have you used Key Vault?",
      "Explain how you deploy applications to Azure Kubernetes Service (AKS).",
      "What tools do you use for monitoring and logging in AKS?",
      "Describe your experience with Helm charts in AKS deployments."
    ];

    let currentQ = 0;
    const synth = window.speechSynthesis;
    const startBtn = document.getElementById("startBtn");
    const recordBtn = document.getElementById("recordBtn");
    const nextBtn = document.getElementById("nextBtn");
    const questionEl = document.getElementById("question");
    const statusEl = document.getElementById("status");
    const recordingsEl = document.getElementById("recordings");

    // Text-to-Speech
    function speak(text) {
      const utter = new SpeechSynthesisUtterance(text);
      utter.rate = 0.9;
      synth.speak(utter);
    }

    // Start Interview
    startBtn.onclick = () => {
      startBtn.disabled = true;
      recordBtn.disabled = false;
      nextBtn.disabled = true;
      nextQuestion();
    };

    // Move to next question
    function nextQuestion() {
      if (currentQ >= questions.length) {
        questionEl.textContent = "‚úÖ Interview complete! Thank you.";
        statusEl.textContent = "You can review your recordings above.";
        recordBtn.disabled = true;
        nextBtn.disabled = true;
        return;
      }

      const q = questions[currentQ];
      questionEl.textContent = `Question ${currentQ + 1}: ${q}`;
      speak(q);
      statusEl.textContent = "Press 'Record Answer' when ready.";
    }

    // Voice Recording Setup
    let mediaRecorder;
    let audioChunks = [];
    let audioURL;

    recordBtn.onclick = async () => {
      recordBtn.disabled = true;
      statusEl.textContent = "üî¥ Recording... (30 seconds)";

      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaRecorder = new MediaRecorder(stream);
        audioChunks = [];

        mediaRecorder.ondataavailable = event => {
          audioChunks.push(event.data);
        };

        mediaRecorder.onstop = () => {
          const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
          audioURL = URL.createObjectURL(audioBlob);

          const audio = document.createElement('audio');
          audio.src = audioURL;
          audio.controls = true;

          const p = document.createElement('p');
          p.innerHTML = `<strong>Q${currentQ + 1}:</strong> ${questions[currentQ]}`;
          recordingsEl.appendChild(p);
          recordingsEl.appendChild(audio);

          statusEl.textContent = "‚úÖ Recording saved. Press 'Next Question' to continue.";
          nextBtn.disabled = false;
        };

        mediaRecorder.start();
        statusEl.textContent = "üî¥ Recording... (30 seconds)";
        
        // Auto-stop after 30 seconds
        setTimeout(() => {
          if (mediaRecorder.state !== "inactive") {
            mediaRecorder.stop();
            stream.getTracks().forEach(track => track.stop());
          }
        }, 30000);

      } catch (err) {
        statusEl.textContent = "Error: " + err.message;
      }
    };

    nextBtn.onclick = () => {
      currentQ++;
      recordBtn.disabled = false;
      nextBtn.disabled = true;
      nextQuestion();
    };
  </script>
</body>
</html>
